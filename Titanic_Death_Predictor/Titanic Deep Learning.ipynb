{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "file_path_gender = \"gender_submission.csv\"\n",
    "file_path_train = \"train.csv\"\n",
    "file_path_test = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "outputs": [],
   "source": [
    "def replace_embarked(embarked):\n",
    "    if embarked == 'Q':\n",
    "        return 1\n",
    "    elif embarked == 'S':\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "outputs": [],
   "source": [
    "def replace_gender(gender):\n",
    "    if gender == 'male':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "outputs": [],
   "source": [
    "def replace_name(name):\n",
    "    if \"Mrs.\" in name:\n",
    "        return 1\n",
    "    elif \"Mr.\" in name:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "outputs": [],
   "source": [
    "def initialise_parameters(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "\n",
    "    for l in range(1, len(layer_dims)):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "    return parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "outputs": [],
   "source": [
    "def sigmoid_backward_propagation(dA, cache):\n",
    "    z = cache\n",
    "    s = 1/ (1 + np.exp(-z))\n",
    "    return dA * s * (1-s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "outputs": [],
   "source": [
    "def leaky_relu(Z):\n",
    "    cache = Z\n",
    "    A = np.maximum(0.01 * Z, Z)\n",
    "    return A, cache;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "outputs": [],
   "source": [
    "def leaky_relu_back_propagation(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0.01\n",
    "    dZ[Z >= 0] = 1\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,Z)\n",
    "    assert(A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "\n",
    "    # When z <= 0, you should set dz to 0 as well.\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "outputs": [],
   "source": [
    "def linear_forward_propagation(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    -- the input of the activation function, also called pre-activation parameter\n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    cache = (A, W, b)\n",
    "    return W.dot(A) + b, cache"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "outputs": [],
   "source": [
    "def forward_propagation(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value\n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward_propagation(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward_propagation(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = forward_propagation(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = forward_propagation(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "\n",
    "    return AL, caches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    # print(AL)\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "\n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "\n",
    "    return dA_prev, dW, db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l\n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward_propagation(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ...\n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters\n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters\n",
    "                  parameters[\"W\" + str(l)] = ...\n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m) * 100))\n",
    "    return p"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "\n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    parameters = initialise_parameters(layers_dims)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender_submission.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(file_path_gender)\n",
    "df_gender = pd.read_csv(file_path_gender)\n",
    "df_train = pd.read_csv(file_path_train)\n",
    "\n",
    "### Let's build our input Matrix\n",
    "\n",
    "X_embarked = df_train.Embarked.map(lambda df: replace_embarked(df))\n",
    "X_gender = df_train.Sex.map(lambda df: replace_gender(df))\n",
    "X_name = df_train.Name.map(lambda df: replace_name(df))\n",
    "X_class = df_train.Pclass\n",
    "X_sib = df_train.SibSp\n",
    "X_parch = df_train.Parch\n",
    "X_fare = df_train.Fare\n",
    "X_age = df_train.Age\n",
    "X_survived = df_train.Survived\n",
    "\n",
    "sns.heatmap(df_train.corr())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.      2.      2.     ...  2.      1.      1.    ]\n",
      " [ 3.      1.      3.     ...  1.      1.      3.    ]\n",
      " [ 2.      1.      3.     ...  3.      2.      2.    ]\n",
      " ...\n",
      " [ 7.25   71.2833  7.925  ... 30.     30.      7.75  ]\n",
      " [ 0.      0.      0.     ...  0.      0.      0.    ]\n",
      " [22.     38.     26.     ... 19.     26.     32.    ]]\n"
     ]
    }
   ],
   "source": [
    "X = pd.concat([X_gender, X_class, X_name, X_embarked, X_sib, X_fare, X_parch, X_age, X_survived], axis=1)\n",
    "# drop the nan values from age\n",
    "X = X[X.Age.notna()]\n",
    "\n",
    "Y = X[\"Survived\"]\n",
    "Y = np.array(Y).reshape((714, 1))\n",
    "X = X.drop(columns=['Survived'], axis=1)\n",
    "X = np.array(X).transpose()\n",
    "\n",
    "print(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693139\n",
      "Cost after iteration 100: 0.676196\n",
      "Cost after iteration 200: 0.640320\n",
      "Cost after iteration 300: 0.630900\n",
      "Cost after iteration 400: 0.628604\n",
      "Cost after iteration 500: 0.627397\n",
      "Cost after iteration 600: 0.625864\n",
      "Cost after iteration 700: 0.622440\n",
      "Cost after iteration 800: 0.629027\n",
      "Cost after iteration 900: 0.616682\n",
      "Cost after iteration 1000: 0.613519\n",
      "Cost after iteration 1100: 0.609560\n",
      "Cost after iteration 1200: 0.606649\n",
      "Cost after iteration 1300: 0.601759\n",
      "Cost after iteration 1400: 0.597183\n",
      "Cost after iteration 1500: 0.597372\n",
      "Cost after iteration 1600: 0.586713\n",
      "Cost after iteration 1700: 0.580615\n",
      "Cost after iteration 1800: 0.603973\n",
      "Cost after iteration 1900: 0.592455\n",
      "Cost after iteration 2000: 0.562842\n",
      "Cost after iteration 2100: 0.583938\n",
      "Cost after iteration 2200: 0.618115\n",
      "Cost after iteration 2300: 0.549948\n",
      "Cost after iteration 2400: 0.575616\n",
      "Cost after iteration 2500: 0.570278\n",
      "Cost after iteration 2600: 0.541176\n",
      "Cost after iteration 2700: 0.656020\n",
      "Cost after iteration 2800: 0.603333\n",
      "Cost after iteration 2900: 0.521493\n",
      "Cost after iteration 3000: 0.578603\n",
      "Cost after iteration 3100: 0.523117\n",
      "Cost after iteration 3200: 0.567743\n",
      "Cost after iteration 3300: 0.564538\n",
      "Cost after iteration 3400: 0.525698\n",
      "Cost after iteration 3500: 0.539814\n",
      "Cost after iteration 3600: 0.516172\n",
      "Cost after iteration 3700: 0.545941\n",
      "Cost after iteration 3800: 0.535019\n",
      "Cost after iteration 3900: 0.533175\n",
      "Cost after iteration 4000: 0.594652\n",
      "Cost after iteration 4100: 0.504232\n",
      "Cost after iteration 4200: 0.544198\n",
      "Cost after iteration 4300: 0.497062\n",
      "Cost after iteration 4400: 0.494925\n",
      "Cost after iteration 4500: 0.502159\n",
      "Cost after iteration 4600: 0.493213\n",
      "Cost after iteration 4700: 0.504389\n",
      "Cost after iteration 4800: 0.512056\n",
      "Cost after iteration 4900: 0.583056\n",
      "Cost after iteration 5000: 0.493462\n",
      "Cost after iteration 5100: 0.548930\n",
      "Cost after iteration 5200: 0.529058\n",
      "Cost after iteration 5300: 0.499386\n",
      "Cost after iteration 5400: 0.495639\n",
      "Cost after iteration 5500: 0.518698\n",
      "Cost after iteration 5600: 0.548309\n",
      "Cost after iteration 5700: 0.518554\n",
      "Cost after iteration 5800: 0.488923\n",
      "Cost after iteration 5900: 0.497603\n",
      "Cost after iteration 6000: 0.499895\n",
      "Cost after iteration 6100: 0.511924\n",
      "Cost after iteration 6200: 0.540456\n",
      "Cost after iteration 6300: 0.483107\n",
      "Cost after iteration 6400: 0.497107\n",
      "Cost after iteration 6500: 0.517925\n",
      "Cost after iteration 6600: 0.530842\n",
      "Cost after iteration 6700: 0.486453\n",
      "Cost after iteration 6800: 0.515843\n",
      "Cost after iteration 6900: 0.479805\n",
      "Cost after iteration 7000: 0.513717\n",
      "Cost after iteration 7100: 0.483975\n",
      "Cost after iteration 7200: 0.510410\n",
      "Cost after iteration 7300: 0.478013\n",
      "Cost after iteration 7400: 0.533365\n",
      "Cost after iteration 7500: 0.504169\n",
      "Cost after iteration 7600: 0.499485\n",
      "Cost after iteration 7700: 0.475445\n",
      "Cost after iteration 7800: 0.483593\n",
      "Cost after iteration 7900: 0.484283\n",
      "Cost after iteration 8000: 0.481946\n",
      "Cost after iteration 8100: 0.486655\n",
      "Cost after iteration 8200: 0.477907\n",
      "Cost after iteration 8300: 0.476124\n",
      "Cost after iteration 8400: 0.502744\n",
      "Cost after iteration 8500: 0.484126\n",
      "Cost after iteration 8600: 0.503981\n",
      "Cost after iteration 8700: 0.505448\n",
      "Cost after iteration 8800: 0.487554\n",
      "Cost after iteration 8900: 0.478066\n",
      "Cost after iteration 9000: 0.476563\n",
      "Cost after iteration 9100: 0.484146\n",
      "Cost after iteration 9200: 0.484421\n",
      "Cost after iteration 9300: 0.483287\n",
      "Cost after iteration 9400: 0.482192\n",
      "Cost after iteration 9500: 0.481494\n",
      "Cost after iteration 9600: 0.478966\n",
      "Cost after iteration 9700: 0.476710\n",
      "Cost after iteration 9800: 0.475479\n",
      "Cost after iteration 9900: 0.477180\n",
      "Cost after iteration 10000: 0.477928\n",
      "Cost after iteration 10100: 0.477802\n",
      "Cost after iteration 10200: 0.477608\n",
      "Cost after iteration 10300: 0.476539\n",
      "Cost after iteration 10400: 0.475090\n",
      "Cost after iteration 10500: 0.473422\n",
      "Cost after iteration 10600: 0.477374\n",
      "Cost after iteration 10700: 0.475863\n",
      "Cost after iteration 10800: 0.474374\n",
      "Cost after iteration 10900: 0.474731\n",
      "Cost after iteration 11000: 0.474161\n",
      "Cost after iteration 11100: 0.474396\n",
      "Cost after iteration 11200: 0.473687\n",
      "Cost after iteration 11300: 0.473053\n",
      "Cost after iteration 11400: 0.474155\n",
      "Cost after iteration 11500: 0.472957\n",
      "Cost after iteration 11600: 0.473375\n",
      "Cost after iteration 11700: 0.473929\n",
      "Cost after iteration 11800: 0.472455\n",
      "Cost after iteration 11900: 0.474053\n",
      "Cost after iteration 12000: 0.473409\n",
      "Cost after iteration 12100: 0.470036\n",
      "Cost after iteration 12200: 0.470658\n",
      "Cost after iteration 12300: 0.470209\n",
      "Cost after iteration 12400: 0.469813\n",
      "Cost after iteration 12500: 0.470804\n",
      "Cost after iteration 12600: 0.468774\n",
      "Cost after iteration 12700: 0.469749\n",
      "Cost after iteration 12800: 0.469266\n",
      "Cost after iteration 12900: 0.468753\n",
      "Cost after iteration 13000: 0.468649\n",
      "Cost after iteration 13100: 0.468687\n",
      "Cost after iteration 13200: 0.467124\n",
      "Cost after iteration 13300: 0.467818\n",
      "Cost after iteration 13400: 0.467530\n",
      "Cost after iteration 13500: 0.466830\n",
      "Cost after iteration 13600: 0.467279\n",
      "Cost after iteration 13700: 0.467678\n",
      "Cost after iteration 13800: 0.467066\n",
      "Cost after iteration 13900: 0.467342\n",
      "Cost after iteration 14000: 0.467629\n",
      "Cost after iteration 14100: 0.466865\n",
      "Cost after iteration 14200: 0.466577\n",
      "Cost after iteration 14300: 0.465426\n",
      "Cost after iteration 14400: 0.465681\n",
      "Cost after iteration 14500: 0.465440\n",
      "Cost after iteration 14600: 0.464775\n",
      "Cost after iteration 14700: 0.464793\n",
      "Cost after iteration 14800: 0.464120\n",
      "Cost after iteration 14900: 0.463904\n",
      "Cost after iteration 15000: 0.463572\n",
      "Cost after iteration 15100: 0.463264\n",
      "Cost after iteration 15200: 0.463230\n",
      "Cost after iteration 15300: 0.463089\n",
      "Cost after iteration 15400: 0.462925\n",
      "Cost after iteration 15500: 0.462837\n",
      "Cost after iteration 15600: 0.462541\n",
      "Cost after iteration 15700: 0.462358\n",
      "Cost after iteration 15800: 0.462155\n",
      "Cost after iteration 15900: 0.461656\n",
      "Cost after iteration 16000: 0.461695\n",
      "Cost after iteration 16100: 0.461298\n",
      "Cost after iteration 16200: 0.461102\n",
      "Cost after iteration 16300: 0.461064\n",
      "Cost after iteration 16400: 0.460453\n",
      "Cost after iteration 16500: 0.460459\n",
      "Cost after iteration 16600: 0.461151\n",
      "Cost after iteration 16700: 0.459822\n",
      "Cost after iteration 16800: 0.459640\n",
      "Cost after iteration 16900: 0.458988\n",
      "Cost after iteration 17000: 0.459139\n",
      "Cost after iteration 17100: 0.459175\n",
      "Cost after iteration 17200: 0.458850\n",
      "Cost after iteration 17300: 0.458698\n",
      "Cost after iteration 17400: 0.458547\n",
      "Cost after iteration 17500: 0.458393\n",
      "Cost after iteration 17600: 0.458239\n",
      "Cost after iteration 17700: 0.458090\n",
      "Cost after iteration 17800: 0.457871\n",
      "Cost after iteration 17900: 0.458092\n",
      "Cost after iteration 18000: 0.456965\n",
      "Cost after iteration 18100: 0.458032\n",
      "Cost after iteration 18200: 0.456275\n",
      "Cost after iteration 18300: 0.457604\n",
      "Cost after iteration 18400: 0.456150\n",
      "Cost after iteration 18500: 0.456052\n",
      "Cost after iteration 18600: 0.456076\n",
      "Cost after iteration 18700: 0.456159\n",
      "Cost after iteration 18800: 0.455974\n",
      "Cost after iteration 18900: 0.455914\n",
      "Cost after iteration 19000: 0.455548\n",
      "Cost after iteration 19100: 0.455480\n",
      "Cost after iteration 19200: 0.455357\n",
      "Cost after iteration 19300: 0.455749\n",
      "Cost after iteration 19400: 0.455410\n",
      "Cost after iteration 19500: 0.455199\n",
      "Cost after iteration 19600: 0.455081\n",
      "Cost after iteration 19700: 0.454973\n",
      "Cost after iteration 19800: 0.454854\n",
      "Cost after iteration 19900: 0.454740\n",
      "Cost after iteration 20000: 0.454625\n",
      "Cost after iteration 20100: 0.454460\n",
      "Cost after iteration 20200: 0.454197\n",
      "Cost after iteration 20300: 0.453952\n",
      "Cost after iteration 20400: 0.453758\n",
      "Cost after iteration 20500: 0.453595\n",
      "Cost after iteration 20600: 0.453664\n",
      "Cost after iteration 20700: 0.453533\n",
      "Cost after iteration 20800: 0.453185\n",
      "Cost after iteration 20900: 0.453199\n",
      "Cost after iteration 21000: 0.453111\n",
      "Cost after iteration 21100: 0.453021\n",
      "Cost after iteration 21200: 0.452930\n",
      "Cost after iteration 21300: 0.452640\n",
      "Cost after iteration 21400: 0.452557\n",
      "Cost after iteration 21500: 0.452474\n",
      "Cost after iteration 21600: 0.452333\n",
      "Cost after iteration 21700: 0.452249\n",
      "Cost after iteration 21800: 0.452307\n",
      "Cost after iteration 21900: 0.452440\n",
      "Cost after iteration 22000: 0.451739\n",
      "Cost after iteration 22100: 0.452209\n",
      "Cost after iteration 22200: 0.451802\n",
      "Cost after iteration 22300: 0.451320\n",
      "Cost after iteration 22400: 0.451515\n",
      "Cost after iteration 22500: 0.451467\n",
      "Cost after iteration 22600: 0.451390\n",
      "Cost after iteration 22700: 0.451315\n",
      "Cost after iteration 22800: 0.451240\n",
      "Cost after iteration 22900: 0.451165\n",
      "Cost after iteration 23000: 0.451092\n",
      "Cost after iteration 23100: 0.451019\n",
      "Cost after iteration 23200: 0.450898\n",
      "Cost after iteration 23300: 0.450827\n",
      "Cost after iteration 23400: 0.450756\n",
      "Cost after iteration 23500: 0.450686\n",
      "Cost after iteration 23600: 0.452415\n",
      "Cost after iteration 23700: 0.451326\n",
      "Cost after iteration 23800: 0.451765\n",
      "Cost after iteration 23900: 0.450979\n",
      "Cost after iteration 24000: 0.451954\n",
      "Cost after iteration 24100: 0.450424\n",
      "Cost after iteration 24200: 0.451351\n",
      "Cost after iteration 24300: 0.451489\n",
      "Cost after iteration 24400: 0.450519\n",
      "Cost after iteration 24500: 0.452198\n",
      "Cost after iteration 24600: 0.450976\n",
      "Cost after iteration 24700: 0.450788\n",
      "Cost after iteration 24800: 0.450677\n",
      "Cost after iteration 24900: 0.450605\n",
      "Cost after iteration 25000: 0.451066\n",
      "Cost after iteration 25100: 0.452121\n",
      "Cost after iteration 25200: 0.451177\n",
      "Cost after iteration 25300: 0.450677\n",
      "Cost after iteration 25400: 0.451776\n",
      "Cost after iteration 25500: 0.450903\n",
      "Cost after iteration 25600: 0.451424\n",
      "Cost after iteration 25700: 0.452031\n",
      "Cost after iteration 25800: 0.451728\n",
      "Cost after iteration 25900: 0.450346\n",
      "Cost after iteration 26000: 0.452145\n",
      "Cost after iteration 26100: 0.451690\n",
      "Cost after iteration 26200: 0.451616\n",
      "Cost after iteration 26300: 0.451429\n",
      "Cost after iteration 26400: 0.451370\n",
      "Cost after iteration 26500: 0.451311\n",
      "Cost after iteration 26600: 0.451253\n",
      "Cost after iteration 26700: 0.451108\n",
      "Cost after iteration 26800: 0.451008\n",
      "Cost after iteration 26900: 0.450985\n",
      "Cost after iteration 27000: 0.450923\n",
      "Cost after iteration 27100: 0.450863\n",
      "Cost after iteration 27200: 0.450805\n",
      "Cost after iteration 27300: 0.450747\n",
      "Cost after iteration 27400: 0.450598\n",
      "Cost after iteration 27500: 0.450486\n",
      "Cost after iteration 27600: 0.450436\n",
      "Cost after iteration 27700: 0.450378\n",
      "Cost after iteration 27800: 0.450292\n",
      "Cost after iteration 27900: 0.450216\n",
      "Cost after iteration 28000: 0.451285\n",
      "Cost after iteration 28100: 0.451057\n",
      "Cost after iteration 28200: 0.450928\n",
      "Cost after iteration 28300: 0.450721\n",
      "Cost after iteration 28400: 0.450660\n",
      "Cost after iteration 28500: 0.450537\n",
      "Cost after iteration 28600: 0.450466\n",
      "Cost after iteration 28700: 0.450400\n",
      "Cost after iteration 28800: 0.450338\n",
      "Cost after iteration 28900: 0.450222\n",
      "Cost after iteration 29000: 0.450142\n",
      "Cost after iteration 29100: 0.450102\n",
      "Cost after iteration 29200: 0.450041\n",
      "Cost after iteration 29300: 0.450112\n",
      "Cost after iteration 29400: 0.449966\n",
      "Cost after iteration 29500: 0.449937\n",
      "Cost after iteration 29600: 0.449872\n",
      "Cost after iteration 29700: 0.449816\n",
      "Cost after iteration 29800: 0.449780\n",
      "Cost after iteration 29900: 0.449705\n",
      "Cost after iteration 30000: 0.449651\n",
      "Cost after iteration 30100: 0.449605\n",
      "Cost after iteration 30200: 0.449539\n",
      "Cost after iteration 30300: 0.449467\n",
      "Cost after iteration 30400: 0.449289\n",
      "Cost after iteration 30500: 0.449446\n",
      "Cost after iteration 30600: 0.449256\n",
      "Cost after iteration 30700: 0.449308\n",
      "Cost after iteration 30800: 0.449056\n",
      "Cost after iteration 30900: 0.448762\n",
      "Cost after iteration 31000: 0.448695\n",
      "Cost after iteration 31100: 0.448663\n",
      "Cost after iteration 31200: 0.448629\n",
      "Cost after iteration 31300: 0.448475\n",
      "Cost after iteration 31400: 0.448445\n",
      "Cost after iteration 31500: 0.448412\n",
      "Cost after iteration 31600: 0.448378\n",
      "Cost after iteration 31700: 0.448343\n",
      "Cost after iteration 31800: 0.448307\n",
      "Cost after iteration 31900: 0.448272\n",
      "Cost after iteration 32000: 0.448235\n",
      "Cost after iteration 32100: 0.448200\n",
      "Cost after iteration 32200: 0.448164\n",
      "Cost after iteration 32300: 0.448129\n",
      "Cost after iteration 32400: 0.448094\n",
      "Cost after iteration 32500: 0.448058\n",
      "Cost after iteration 32600: 0.448024\n",
      "Cost after iteration 32700: 0.447990\n",
      "Cost after iteration 32800: 0.447956\n",
      "Cost after iteration 32900: 0.447922\n",
      "Cost after iteration 33000: 0.447889\n",
      "Cost after iteration 33100: 0.447856\n",
      "Cost after iteration 33200: 0.447823\n",
      "Cost after iteration 33300: 0.447791\n",
      "Cost after iteration 33400: 0.447758\n",
      "Cost after iteration 33500: 0.447727\n",
      "Cost after iteration 33600: 0.447695\n",
      "Cost after iteration 33700: 0.447663\n",
      "Cost after iteration 33800: 0.447633\n",
      "Cost after iteration 33900: 0.447602\n",
      "Cost after iteration 34000: 0.447527\n",
      "Cost after iteration 34100: 0.447496\n",
      "Cost after iteration 34200: 0.447469\n",
      "Cost after iteration 34300: 0.447389\n",
      "Cost after iteration 34400: 0.447357\n",
      "Cost after iteration 34500: 0.447325\n",
      "Cost after iteration 34600: 0.447294\n",
      "Cost after iteration 34700: 0.447264\n",
      "Cost after iteration 34800: 0.447162\n",
      "Cost after iteration 34900: 0.447057\n",
      "Cost after iteration 35000: 0.447188\n",
      "Cost after iteration 35100: 0.446998\n",
      "Cost after iteration 35200: 0.447127\n",
      "Cost after iteration 35300: 0.446936\n",
      "Cost after iteration 35400: 0.447235\n",
      "Cost after iteration 35500: 0.447141\n",
      "Cost after iteration 35600: 0.447119\n",
      "Cost after iteration 35700: 0.447388\n",
      "Cost after iteration 35800: 0.447258\n",
      "Cost after iteration 35900: 0.447704\n",
      "Cost after iteration 36000: 0.447278\n",
      "Cost after iteration 36100: 0.447182\n",
      "Cost after iteration 36200: 0.447577\n",
      "Cost after iteration 36300: 0.447600\n",
      "Cost after iteration 36400: 0.447845\n",
      "Cost after iteration 36500: 0.447392\n",
      "Cost after iteration 36600: 0.447367\n",
      "Cost after iteration 36700: 0.447599\n",
      "Cost after iteration 36800: 0.447522\n",
      "Cost after iteration 36900: 0.447890\n",
      "Cost after iteration 37000: 0.447569\n",
      "Cost after iteration 37100: 0.447368\n",
      "Cost after iteration 37200: 0.447553\n",
      "Cost after iteration 37300: 0.447939\n",
      "Cost after iteration 37400: 0.447457\n",
      "Cost after iteration 37500: 0.447318\n",
      "Cost after iteration 37600: 0.447253\n",
      "Cost after iteration 37700: 0.447995\n",
      "Cost after iteration 37800: 0.447811\n",
      "Cost after iteration 37900: 0.447776\n",
      "Cost after iteration 38000: 0.447745\n",
      "Cost after iteration 38100: 0.447716\n",
      "Cost after iteration 38200: 0.447688\n",
      "Cost after iteration 38300: 0.447662\n",
      "Cost after iteration 38400: 0.447635\n",
      "Cost after iteration 38500: 0.447610\n",
      "Cost after iteration 38600: 0.447584\n",
      "Cost after iteration 38700: 0.447559\n",
      "Cost after iteration 38800: 0.447534\n",
      "Cost after iteration 38900: 0.447509\n",
      "Cost after iteration 39000: 0.447484\n",
      "Cost after iteration 39100: 0.447460\n",
      "Cost after iteration 39200: 0.447436\n",
      "Cost after iteration 39300: 0.447411\n",
      "Cost after iteration 39400: 0.447387\n",
      "Cost after iteration 39500: 0.447304\n",
      "Cost after iteration 39600: 0.447279\n",
      "Cost after iteration 39700: 0.447253\n",
      "Cost after iteration 39800: 0.447189\n",
      "Cost after iteration 39900: 0.447055\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2DklEQVR4nO3deZxcdZnv8c9TWy/pJZ2ksweSQGLYt7CJOIDsKDBug44K3nEY5w46V1+jF64zwuA44jYuM4wMoxF3VNQxKIpR2RUhQIAkkIUkkASydie9L1X93D/OqepT1dWd7qSrt3zfr1e9us7vnFP11Omknv6tx9wdERGRQrHRDkBERMYmJQgRESlKCUJERIpSghARkaKUIEREpCglCBERKUoJQg5bZnauma0b7ThExiolCBkVZrbFzC4czRjc/RF3f91oxpBlZueZ2bYReq83mdmLZtZmZg+Y2ZEDHDs/PKYtPOfCgv0LzewXZtZsZnvM7HOl/wQyUpQgZMIys/hoxwBggTHxf83MpgE/Bf4JmAKsBH44wCk/AJ4BpgKfAO4xs/rwtVLACuD3wExgLvDdkgUvI25M/KMVyTKzmJndaGYvmdleM/uRmU2J7P+xme0ws/1m9rCZHRfZd5eZfc3M7jOzVuD8sKbyD2b2XHjOD82sPDw+76/2gY4N93/czF4zs1fN7ANm5mZ2dD+f40Ez+7SZPQa0AQvN7P1m9kL41/YmM/ub8NhJwK+A2WbWEj5mH+haHKS3Amvc/cfu3gHcApxkZkuKfIbFwKnAze7e7u4/AZ4H3hYech3wqrv/m7u3unuHuz93iPHJGKIEIWPNh4CrgT8DZgONwO2R/b8CFgHTgaeB7xWc/27g00A18GhY9k7gUmABcCLBF1t/ih5rZpcCHwUuBI4GzhvEZ3kvcH0Yy8vALuDNQA3wfuBLZnaqu7cClxF82VaFj1cHcS1yzOwIM9s3wOPd4aHHAc9mzwvf+6WwvNBxwCZ3b46UPRs59ixgi5n9KmxeetDMThjEdZFxIjHaAYgU+CBwg7tvAzCzW4BXzOy97p5292XZA8N9jWZW6+77w+Kfu/tj4fMOMwP4aviFi5ndC5w8wPv3d+w7gW+6+5rIe//lAT7LXdnjQ7+MPH/IzH4DnEuQ6IoZ8FpED3T3V4DJB4gHoArYXVC2nyCJFTt2f5Fj54TP5wLnA1cCvwP+Hvi5mS1x965BxCJjnGoQMtYcCfws+5cv8AKQAWaYWdzMbgubXJqALeE50yLnby3ymjsiz9sIvvj609+xswteu9j7FMo7xswuM7PHzawh/GyXkx97oX6vxSDeuz8tBDWYqBqg+SCObQcedfdfhQnhCwR9FcccQnwyhihByFizFbjM3SdHHuXuvp2g+egqgmaeWmB+eI5Fzi/V8sSvEfzFnDVvEOfkYjGzMuAnBF+iM9x9MnAfvbEXi3uga5EnbGJqGeCRre2sAU6KnDcJOCosL7SGoO8kWrs4KXLsc/3ELROEEoSMpqSZlUceCeAO4NMWDr00s3ozuyo8vhroBPYClcC/jmCsPwLeb2bHmFklwSigoUgBZQTNO2kzuwy4OLJ/JzDVzGojZQNdizzu/kqk/6LYI9tX8zPgeDN7W9gB/0ngOXd/schrrgdWATeHv58/J+iX+Ul4yHeBs8zsQgtGjP0fYA9BTUcmACUIGU33ETRTZB+3AF8BlgO/MbNm4HHgzPD4bxN09m4H1ob7RoS7/wr4KvAAsDHy3p2DPL8Z+DBBomkkqA0tj+x/kWBI6aawSWk2A1+Lg/0cuwlGIX06jONM4JrsfjO7w8zuiJxyDbA0PPY24O3ha+Du64D3ECSyRoLa3ZXqf5g4TDcMEhk6MzsGWA2UFXYYi0wUqkGIDJKZ/bmZlZlZHfBZ4F4lB5nIlCBEBu9vCOYyvEQwmuhvRzcckdJSE5OIiBSlGoSIiBQ1YWZST5s2zefPnz/aYYiIjCtPPfXUHnevL7ZvwiSI+fPns3LlytEOQ0RkXDGzl/vbV9ImJjO71MzWmdlGM7uxyP4vmdmq8LE+XE4gu+9aM9sQPq4tZZwiItJXyWoQ4czK24GLgG3Ak2a23N3XZo9x949Ejv8QcEr4fApwM8EEHQeeCs9tLFW8IiKSr5Q1iDOAje6+KZxZeTfBTMv+vItgJinAJcAKd28Ik8IKgiWYRURkhJQyQcwhfzXLbfQuE5wnXGtmAcGdqQZ9rpldb2YrzWzl7t2FKxiLiMihGCvDXK8B7nH3zFBOcvc73X2puy+try/aCS8iIgeplAliO/lLIs8Ny4q5ht7mpaGeKyIiJVDKBPEksMjMFlhwc/NriKxemRXeC7cO+GOk+H7gYjOrC9e9uTgsExGREVKyBBEuYnYDwRf7C8CP3H2Nmd1qZldGDr0GuNsja364ewPwKYIk8yRwa1g27Jo7uvnSivWs2rqvFC8vIjJulXSinLvfR7Dmf7TskwXbt/Rz7jJgWbF9wynT43zldxuorUhy8rzJpX47EZFxY6x0Uo+aSWVBjmzp1KrNIiJRh32CSMZjVCTjShAiIgUO+wQBUFWeoLmje7TDEBEZU5QggOqyBM0dqkGIiEQpQQDV5Qk1MYmIFFCCINvEpAQhIhKlBAFUlSVoUYIQEcmjBAFUlSXVxCQiUkAJgqAPokmjmERE8ihB0NtJHVntQ0TksKcEQdAH4Q5tXUNabVxEZEJTggCqy5MAGskkIhKhBEEwzBXQbGoRkQglCKA8EVyGju6eUY5ERGTsUIIAUmGC6MooQYiIZClBEEkQaSUIEZEsJQigTDUIEZE+lCCAVDwOqAYhIhKlBAEkEwZAt2oQIiI5ShBAKq4+CBGRQkoQqJNaRKQYJQh6E0SnmphERHKUIICysJO6WzUIEZEcJQh6O6k1zFVEpJcSBOqkFhEpRgkCSMRjxEwJQkQkSgkilErE1MQkIhKhBBFKxWOqQYiIRChBhIazBvHijia+/Nv1w/JaIiKjRQkiNJw1iHd87Y98+bcb6OjWLUxFZPxSggilEsOXILp7gtfpcR+W1xMRGQ0lTRBmdqmZrTOzjWZ2Yz/HvNPM1prZGjP7fqQ8Y2arwsfyUsYJQYIYrsX6YhbMq0j3KEGIyPiVKNULm1kcuB24CNgGPGlmy919beSYRcBNwDnu3mhm0yMv0e7uJ5cqvkLDWYOIZxNERglCRMavUtYgzgA2uvsmd+8C7gauKjjmr4Hb3b0RwN13lTCeASXjw9dJHYtlaxAaFSUi41cpE8QcYGtke1tYFrUYWGxmj5nZ42Z2aWRfuZmtDMuvLvYGZnZ9eMzK3bt3H1KwqXiMzmGqQYT5QTUIERnXStbENIT3XwScB8wFHjazE9x9H3Cku283s4XA783seXd/KXqyu98J3AmwdOnSQ/o2TiViNHekD+UlcuIxNTGJyPhXyhrEdmBeZHtuWBa1DVju7t3uvhlYT5AwcPft4c9NwIPAKSWMlbJh7KQ2UxOTiIx/pUwQTwKLzGyBmaWAa4DC0Uj/Q1B7wMymETQ5bTKzOjMri5SfA6ylhErSSa1RTCIyjpWsicnd02Z2A3A/EAeWufsaM7sVWOnuy8N9F5vZWiADfMzd95rZ64H/MrMegiR2W3T0UykMaye1+iBEZAIoaR+Eu98H3FdQ9snIcwc+Gj6ix/wBOKGUsRUazpnUamISkYlAM6lDwzlRLttJ3a0ahIiMY0oQobJEnI7u4U0QGfVBiMg4pgQRmlqVoqUzPSwL7FmuD0JNTCIyfilBhKZXlwGwq6nzkF9Lo5hEZCJQggjNqCkHYFdzxyG/Vkyd1CIyAShBhLIJYucw1CBi6qQWkQlACSKUbWLa2TQcNYjgpzqpRWQ8U4IITa5MkorH2DmMTUzDNWxWRGQ0KEGEzIzpNWXD0kkd02J9IjIBKEFEHFVfxRObGw65aUhNTCIyEShBRFxz+jy272tnxdoduDvzb/wltz+wccivkx3m2q1RTCIyjilBRFx47AyOnl7FJ362mue37wfg8/evG/LrxDSTWkQmACWIiGQ8xh3vOZXmzjT/666VQNB5PVTZJiYNcxWR8UwJosDR06v5wBsWsKcl6KyurTiYBJHtpFYTk4iMX0oQRVx+wqzc8/JEfMjnx7TUhohMAEoQRRw7qyb3fH9795DP1zBXEZkIlCCKiMWMf/3zE6guT9DY1kVwX6PBC7sgyGgUk4iMY0oQ/Xj3mUfwd+cfTWe6h/YhLgHeEyaUbjUxicg4pgQxgLpwBFNDa9eAx7k7m3a3RLaDn+qkFpHxTAliAHWVKQD2tQ3cD/GDJ7ZywRcf4onNDUBvDUKd1CIynilBDGBqVZAgXts/8AJ+T7/SCMCWPa1AJEGok1pExjEliAEcN7uW8mSMRzbsHvC47Izp7OilbN+0ahAiMp4pQQygPBnnDUfX87sXdg04kimbCBLZBJGrQagPQkTGLyWIAzh/ST3b97WzZW9bv8f0FNYg1AchIhOAEsQBnLlgCgBPbN7b7zGZPjWIoFwJQkTGMyWIAziqvoopk1I8sbmx32OyiSAeJghXE5OITABKEAdgZpy5YAqPbtyda0oqlG1Syt4HInuYVnMVkfFMCWIQLj5uBjubOnlm676i+7M1iGyiyP7UUhsiMp4pQQzCm46ZQTJu/GbtjqL7szWLTC5RBOXqgxCR8UwJYhBqypMcO7uWVa/sK7o/mxgy2RpEjybKicj4pwQxSCfOqWXNq025L/9VW/dx+qd/S2NrV2+CKGhqSquJSUTGsZImCDO71MzWmdlGM7uxn2PeaWZrzWyNmX0/Un6tmW0IH9eWMs7BOGFOLS2daTbvDZbT+M8HNrK7uZPHN+3NJYJsjUHzIERkIkiU6oXNLA7cDlwEbAOeNLPl7r42cswi4CbgHHdvNLPpYfkU4GZgKeDAU+G5/Y81LbET5tYCsHr7fo6qr6IyFdxprq0r06eJKTvpur9RTyIi40EpaxBnABvdfZO7dwF3A1cVHPPXwO3ZL3533xWWXwKscPeGcN8K4NISxnpAi6ZXUZaI8fy2/QBUpILc2tadySWGwiamzBBvNCQiMpaUMkHMAbZGtreFZVGLgcVm9piZPW5mlw7hXMzsejNbaWYrd+8eeEG9Q5WIxzh2dg3PbQ8TRDKsQXSmc01LhaOYNE9ORMaz0e6kTgCLgPOAdwH/bWaTB3uyu9/p7kvdfWl9fX1pIow4YU4tT2xu4KdPbyMeXrmGtq7IvIeC+RBqYhKRcayUCWI7MC+yPTcsi9oGLHf3bnffDKwnSBiDOXfE/dniIAndvHxN7jakDS19RzFlW5bUxCQi41kpE8STwCIzW2BmKeAaYHnBMf9DUHvAzKYRNDltAu4HLjazOjOrAy4Oy0bVm46Zwf8+7yjauzK0dYYJosgw11xN4gA1iJ1NHbmbDYmIjDUlG8Xk7mkzu4Hgiz0OLHP3NWZ2K7DS3ZfTmwjWAhngY+6+F8DMPkWQZABudfeGUsU6FNOqykj3ODuagrvM7W3tytUUCpfcOFAN4qJ/e4imjjRbbruihBGLiByckiUIAHe/D7ivoOyTkecOfDR8FJ67DFhWyvgORvY2pK80BPeH2NvaSaZg/kNvJ/XACaKpI12iKEVEDt1od1KPO/VVZQBsa2wHoLkjnas5ZEczuTqpRWQCKGkNYiKaVl2Wt93Z3UMsXOY7U7iaqzqpRWQcU4IYomlVBQkincndKCi7vLfmQYjIRKAmpiGaXJHMJQQIkkFrV9CXkE0IhfeFEBEZj5QghigWM6ZOSuWV5eY9hDUIH2QntYjIWKYEcRCOnFoJQCqRf/myNYjBzoPIctU0RGQMUoI4CAunVQFQV5nMK+/tgxhaJ3WxmkZPj6sGIiKjSgniIMybUlG0PF241MYgv+CLJZI3/dtDHPNPvz64AEVEhoFGMR2EWbVBgmho7corL+ycHmwndbHDNu9pPYQIRUQOnWoQB+HY2TUALJ5RnVdeeEe5Qdcg1JQkImPQoBKEmb1jMGWHi2Nm1XDvDW/g45cuySvPFCy10ePwrT9soSNc+bU/mlAnImPRYGsQNw2y7LBxwtxaqsryW+gyPZ4bkZSdKnHz8jU8tnHPgK/lmlAnImPQgH0QZnYZcDkwx8y+GtlVAxz2K82VFQxz7ejO0JkOvu0T8Rhd4fPuA0ypVg1CRMaiA3VSvwqsBK4EnoqUNwMfKVVQ40V5sjdBlCVi3L9mJ2d95ncApPISxMAJQH0QIjIWDZgg3P1Z4Fkz+767dwOEN/CZ5+6H/Z1uyhLx3PPq8iSdLZ3sa+sGIBHvXY7jQAlAS3KIyFg02D6IFWZWY2ZTgKcJ7h39pRLGNS6URWoQNeX5uTYR6913oCYmJQgRGYsGmyBq3b0JeCvwbXc/E3hT6cIaH6I1iEkFHdbJIdQg1MQkImPRYBNEwsxmAe8EflHCeMaVaCd1NCFAfhNT+kBNTBrFJCJj0GATxK0E949+yd2fNLOFwIbShTU+RBNEtEkJIBnZTquJSUTGoUEtteHuPwZ+HNneBLytVEGNF2a9tYSC/JBXg7j7ya3saOrkxsvyJ9ZlaZiriIxFg51JPdfMfmZmu8LHT8xsbqmDG08K+xGiNYoXdzRzx0Mv0dzRXfRc3btaRMaiwTYxfRNYDswOH/eGZRLqKpjrUNgnAfDohuIzqpUfRGQsGmyCqHf3b7p7OnzcBdSXMK5xpzud38+QiPe9tH/a3FD0XI1iEpGxaLAJYq+ZvcfM4uHjPcDeUgY23hTOdUjE+tYg2rqKr06iTmoRGYsGmyD+F8EQ1x3Aa8DbgetKFNO4cud7T+OeD57dJ0Eki9Qg+ltyQzUIERmLBnvDoFuBa7PLa4Qzqr9AkDgOaxcfNxPo++WfKNIH0ZUuPtxVNQgRGYsGW4M4Mbr2krs3AKeUJqTxqW8TU99L2xU5JjpySQlCRMaiwSaIWLhIH5CrQeh2pRF9m5j61iCix0RnVw80j05DYEVktAz2S/6LwB/NLDtZ7h3Ap0sT0viUbWKKx4xMjxcdxRRtYsoMsgbR406MvslGRKTUBjuT+ttmthK4ICx6q7uvLV1Y40+2+ai6PMG+tu5B1CCKNzcBubvSQTDLWlU1ERkNg/7uCROCkkI/sl/+NeXJIEEU7YOINivlJ4GoaL7QQn4iMloG2wdxUMzsUjNbZ2YbzezGIvuvM7PdZrYqfHwgsi8TKV9eyjiHQ/Y7vjq8L0SxUUzRyXTpvCam/OOitQut0yQio6VkrRdmFgduBy4CtgFPmtnyIk1TP3T3G4q8RLu7n1yq+EqlpjwJFJ8HER3FlNcHUZAhorUGjXASkdFSyhrEGcBGd9/k7l3A3cBVJXy/MWFSWXAToXiRmdT9j2LKTwLRWoNGMYnIaCllgpgDbI1sbwvLCr3NzJ4zs3vMbF6kvNzMVprZ42Z2dbE3MLPrw2NW7t69e/giPwj/eMUxTK8uIxXeI6LYH/7RJqZMpv8+iOi+m376PHtaOsPXdHbs7xjOsEVE+lXSPohBuBeY7+4nAiuAb0X2HenuS4F3A182s6MKT3b3O919qbsvra8f3bUDP3DuQp74xIW5pqVMkd7lrn6SghcmiMj2r1bv4FO/CFrlHli3izd89vfsbu4c1thFRIopZYLYDkRrBHPDshx33+vu2W+7rwOnRfZtD39uAh5knMzcTmUTRJEqRFc6A8DWhra8L/nCiXLpguTS3hWct2N/J+kep7GtazhDFhEpqpRD7J8EFpnZAoLEcA1BbSDHzGa5+2vh5pXAC2F5HdDm7p1mNg04B/hcCWMdNolcDaLvvuxkunM/90BeeZ9hrgXndoZNU51hguns1thXESm9kiUId0+b2Q0E97KOA8vcfY2Z3QqsdPflwIfN7EogDTTQu0LsMcB/mVkPQS3ntvEyMS8VDm8t7FyuTMX7Xazvwz94hrgZV5w4C+hbg8glhoJEISJSSiWdpOvu9wH3FZR9MvL8JuCmIuf9ATihlLGVSraTurBWUJlK0NbV2e+opJuXr84liH5rEN09edsiIqU02p3UE062k7owEWSHv3b1uzJf77DYwuSSTQwduZqEahAiUnpKEMPszSfOBuDi42bklVemgsraQ+uLD8e1yLSJwhFQhX0PHZE+iA/94BluWb7m0IIWESlCCWKYHTu7hi23XcGxs2rzyitTQQ3ib77zVNHzotPqCisZfTqpIzWIe599lbv+sOXQghYRKUIJokQK1+rLJoj+RGsQfTup8/seNIpJREaCEkSJFC61MSk18HgAi9QhCjupuwoThDqpRWQEKEGUSNzyE8RQahCFndQd3Zm8n+qkFpGRoARRIrGCGkRl2QESROR5307q/JpDxxCbmNq7MuzT7GsRGSIliBIprEEcqIkpqr+RsJ0HWYO44quPcPKtK4Z0joiIEkSJ9KlBFCSIO95zGl+55uTctkUSSmEndVZhJ3W6IJO0daVp7Uz3OW/TntbBBy4iEtLtjkuksJO6sA+ipiLBqUcUX4G2WH7ozvT0Hc1U0Fl90j//hkyPs+kzVxxs2CIiOUoQJdKnk7qgD6IsEScWOWagYa4AbZ2ZPk1MhQmiO6ObC4nI8FETU4kUzoOoSBYmiFheLSOaIIrdZrQjnSnSWT20vojC+06IiAxECaJEYgU1iMKVXMuTMaKtUNF5EMU6qTu7e3I1h97hrkMbzdT/OlAiIn0pQZRI3yam/Na8skQ8r2N6oLWYIGhWKlzNNVqDGExtQhPsRGQolCBKpHAU00lza1l23dLcdlky/9IPtBYTBHMfBlr2e19b9wFjGmqTlIgc3pQgRkhZIs4FS2bkbUcdaJhrW1c610RU2NQEDOo2pFrDSUSGQgliBHztL09lZm15XllZov8aRLFO6uaO3vkNxZqYBlODUBOTiAyFhrmW0LvOmMfFx83k/NdN77OvMEFEM0S6yHDVpo7eBFCsk3owS2moiUlEhkIJooQ+89YT+91nBZ3Y0UpDsRpEU3tvgihWg2hUDUJEhpmamMaI7kjPdLFO6v3tQRNTeTJWdCb1oPogtAqsiAyBEsQYkZ8g+maIbAKoq0z1zqjO64PoTRAtRdZjCo5XDUJEBk8JYoRdfOyMouXRfodMT98mpmwCqK1IFq1BNLX3JoXjb76fFWt39nkN1SBEZCjUBzHC7nzf0qLl0RpEukiCyPYx1FWmeDHdjLvn9UFEO7EBvvv4y8yqLef4Ob33xlYfhIgMhWoQY0Q0KRTrpM7WIOomJYHgyz5646DCBPHQ+t28+d8fzStTE5OIDIUSxBgRbWIqXJU1HjP2haOYJlemgGyCiNQg2ov3O0R1qIlJRIZACWKM6I50TO/Y35G3rywRo7E1qEFMrsjWIDLsjwx9LaxBZF3/7ZW556pBiMhQKEGMEe69ndPbGtuYFZl5XZaI0RTOpK7L1iC6e/KGtkbnSUT9JtJZrU5qERkKJYgxJNtRvX1fO/OmVObKo+s21Vb29kE0tEYSRMcgmphUgxCRIVCCGEO6Mz24O9sa2zkykiDKw5VfzaCmvLeJKTp7OtPjVJcNPChNNQgRGQoliDEknXEa27pp68pwRJEaREUynksWHd1BDWJaVVnuuJqwf6I/Bxrm+p8PbuRrD750sOGLyASjBDGGdPf0sK2xDYAjpkYSRJgUKlNxysNbl7Z2ptnf3k19dW+CqD1Aghhosb6nXm7kc79ex2d//eJBxy8iE0tJE4SZXWpm68xso5ndWGT/dWa228xWhY8PRPZda2Ybwse1pYxzNP37u07hihNmAdDSkWZbYzsAM2vyO6kBypPx3POdTcFIp+lDSBAv720rWr7s0c1887HNAEyrSh3MxxCRCahkM6nNLA7cDlwEbAOeNLPl7r624NAfuvsNBedOAW4GlgIOPBWe21iqeEfLW06aTVe6h18+/xoXfPGhXPn0vAQR1BoqU/Hc8+xQ2GiCmFw5cIJYvX0/PT2ed7e7rnQPt/6i91dSZBK3iBymSlmDOAPY6O6b3L0LuBu4apDnXgKscPeGMCmsAC4tUZyjLhG3PmXRpqNsv0NFMp5rbnotrEEMtonphDm1tHZl2LSnNa+8cPJcyyBGQ4nI4aGUCWIOsDWyvS0sK/Q2M3vOzO4xs3lDOdfMrjezlWa2cvfu3cMV94hLxvN/DRXJOJNSvUNbc53Uqd4mplWv7ANgbl1vX8VACeKUIyYDsObV/Xnlhf0SXZkejXYSEWD0O6nvBea7+4kEtYRvDeVkd7/T3Ze6+9L6+vqSBDgSErH8GkQqEcu7odBR06uAYChrNlmsfa2Jk+bWsnhGVe64gUYxZW95Wnhr0mKzq1s7lSBEpLQJYjswL7I9NyzLcfe97t4Zbn4dOG2w504kZcl43nZ3wR2Dzl44FYDnt+/PNTcBXH3KHOKR5DJQDaI6nD/RXlBjKDayqbWf+0mIyOGllAniSWCRmS0wsxRwDbA8eoCZzYpsXgm8ED6/H7jYzOrMrA64OCybkE47si5vu/Ce1NnmoZhZ3qzq+VMnkYj1/goHGoFUVRac196VnxAKEwZAs/ohRIQSjmJy97SZ3UDwxR4Hlrn7GjO7FVjp7suBD5vZlUAaaACuC89tMLNPESQZgFvdvaFUsY62qrIEbz1lDj99JqgkdRfcUa48GeeO95zKkVMnkYx0aM+sLc+7l3V00lyhZDxGeTLWp8YQXX6jpjxBU0e63zvSicjhpaQ3DHL3+4D7Cso+GXl+E3BTP+cuA5aVMr6x5AvvOIlPvuVYTr51BQumTcqVL5lZDcClx8/qc86s2nJ2N3fmtgdKEIlYjIpkfMAmphk15TR1tKiJSUQA3VFuzIjFjMmVKf7rvadx0tzJADz8sfNzNwgqprYimbdg30Cd1ImYUZGM09Y1cILYsKuFZiUIEUEJYsy55LiZuefR5TaKMbO8PohkkfkUWfG4UZ4qUoOIrM+UnXSnGoSIwOgPc5VDFI8khcL5FFHJsImpY4AaRPZudZosJyKgGsS4dO6iaSwM+ymSscEliHjYxDRQH8SkcKSTmphEBJQgxqXv/NWZuefReRDxWP9NTMm4UZGK50Yo7Wnp5Av3r8tNoAMwoLo80e/d6UTk8KIEMc5F+yAAjphSySsNfVdtzdYgsqOePnPfi/zk6W0cVd87YsoJ+iF2NXf0OV9EDj/qgxjn4gUd0w9//Py8pcKzErEYFak4O5s6+Pg9z/KTp7cB5M15cIfp1eXsbOrkF8+9ygVfeFD3hxA5jClBjHPZdZyiq7o6wey56PLfiXhQg2hs6+ZHK7flync2BTWKC5ZM57pz5jOjJqhB3P3EVjbtaeVrD77Ed/64ZQQ+iYiMNWpiGufKk3FuecuxnL9kep99v/r7c3nP1//ES7tbiccsdze6QtOqUiy77nQgmAuxs6mTmBmXnzCTzu4ebl6+hs50Dx84d2FJP4uIjC2qQUwA152zgCOnTupTHovMk+hxpyJcQvzCY2bwL1cfT11Yw4iu71RfXUZXuoeX97ZxxJRJ/Pu7T+GCJdO57Vcvqm9C5DCjBDEB/dniYOnzilQ8d/e4TI9TEdYgZtSU8Z6zjuTkeZNzx2XNiPRfzK2roDKV4P9dfgzpHueep3qbpkRk4lOCmID+5eoTePhj51NTnszdYMgd0uH9RKdOCibEzaytAMhbQrwwQQAsrK/izAVTuPuJrXSmM3xpxXo+8sNVfOPRzbmbC63b0Yy77lcqMpEoQUxAqUQst0zHv7/rFP7qDQs4dlYNjeG6TXVhgsguM756e1Pu3ONm1+SeR+9W9+4zj+CVhjYu+MJDfOV3G3hs4x4+9Yu1/MOPn2PllgYu+fLDfOsPW0r90URkBClBTHDzplTyT28+lljMaO4IJsDVhUtqXH3ybACOn9ObFCaVJVh23VJef9RUjpjSmyAuOW4m06vL2L6vneteP58nPnEh/3DxYu599lU+9cvgNh6fv38dG3Y2k870vUudiIw/NlGaBZYuXeorV64c7TDGtG2NbXzh/nXc9rYTcyOa9rd1YzGoKe9/Jdis9Tub+cYjm7np8iVMrkyRzvRw5X88xtrXmpgzuYLdzZ10ZXqYOinFEVMr+dAFR/PIhj2847R5rN/ZTG1lkmdebuSaM45g9uSKUn9cERkEM3vK3ZcW3acEIYfimVcaeevX/sB1r5/PmQum8ujG3WzZ08YLrzWxN7IUedTMmnIe/b/nkxhg7SgRGRkDJQjNg5BDcsoRddz912exaEY1UyaluPT4YLny1s40X/39BmJmbNjZwrmLpnHz8jUsrJ/Ept2tPLttf59brYrI2KIahIyYpo5uMhnnlE+tAOCdS+eyt6WLqvIEt7zlOCZXJjHrf8FBERl+qkHImJDt53jj4nqe3bqP5c++mrsn9s9XvcqZC6bwvrPnM60qxenzp+TmcIjI6FANQkZcd6aHuBn727vZuLuFv/ve08ytq+CVhnb2tARrQx0/p4aLjpnJ1afMZvu+dva3dTO3rpJNe1q45LiZ/S4bIiJDo05qGdM6ujOUJWJ0pnvYsLOFT9+3lsc3NQAQM+gp+Cd6VP0kPnrR67jixFmjEK3IxKIEIeNKU0c3D7y4i1Pm1bHssc0cO6uGmookbV1pUokYX/3dBtbvbGF2bTmvP3oa7zv7SI6ZVTPgHfVEpDglCJlQWjrTfOSHq0hnenjspb10pXtYMrOaa18/nwuWTM9bLkREBqYEIRPWy3tbeXDdbr7+6Ca2NrQDwTyLGbXlHDurhuPn1HD87FqWzKrOW7VWRAJKEDLhuTvrd7bw2xd2snlPK9sb21n7WhP7w/tr15QnOGvhVM5aOJVTj6zjqPpJpBIxdjV1Mi+ypIjI4UbDXGXCMzNeN7Oa182szpW5O9sa21m9fT8/e2Y7a19r4jdrdwJBwqguT/La/naWXXc6f7a4XnMwRAqoBiGHlU27W3hxRzP3PLWNbY1trN/ZAsDZC4PFCSeVJTj7qKm8bkY1c+sqNBdDJjw1MYn0Y2dTB/c8tY2vP7KJRDzG/vZuutLB5L2KZJzFM6qYXlNOZSpOVVmCC4+ZQVV5gtPnTxnlyEWGhxKEyCC1daV5cUcz63c0s35nC+t3NrNuZzO7mzvzjvvSX5zEounVZHqcBfWTBrUarshYpAQhcoiWP/sqj23Yww9Xbi26f1ZtOYtnVLOvvZtkzDh3UT1HTZ/EvLpK5k2ppE7rTMkYpQQhMkyaO7rp6YGnX2mkM92DGby0uyVX40jGjR6H57fvzzuvqizB4hlVnLFgKqfPr2PKpBSTK1PMn1qpxCGjatRGMZnZpcBXgDjwdXe/rZ/j3gbcA5zu7ivNbD7wArAuPORxd/9gKWMVGYzqsCnp/CXTBzyutTPN1sY2tja080pDG1sb2nhu2z6+/sgm7nio94+yWbXl1FWmqJuUZHZtBScfMZkplSlqK5NMrkiR7ulh8YzqIa09tbOpg/qqsqId7D09ro53GbSSJQgziwO3AxcB24AnzWy5u68tOK4a+HvgTwUv8ZK7n1yq+ERKaVJZgiUza1gysyavvK0rzertTbR0drN9XwdPbm6grStNQ2sXv31hJz9+alvR16tMxXnrqXOYUV1ORSrO5MoUdZVJJlemmDopxdSqFFVlCTbsauHiLz1MVVmCNy6exq6mTt522lzSmR460z3854Mv8ck3H8vVp8yhp8e56afPs/rV/dx42RKWr3qV046s464/bGFuXQVNHWn2tnRy1sKpTKsqo746eMyZXMHM2nKqyhKUJWJFa0BtXWlWbd3H0fVBJ7+MTyVrYjKzs4Fb3P2ScPsmAHf/TMFxXwZWAB8D/iFSg/iFux8/2PdTE5OMd5keZ0dTB/vbutnf3s2+ti52t3Ty69U7qJuU4v7VO0gXrlwYkYrHSMaN1q7MAd+rvrqMVDzG9n3tffbNm1KRm5UOUFeZpLGtu+jrJONGbUWSmbXlTJ1UxpRJKaZMSvHE5oZcM9u0qjJm1ZYzrSrF1KoyplWVMWVSktqKJOXJOD3uYVmKusrgfK3WO3JGq4lpDhDt0dsGnFkQ2KnAPHf/pZl9rOD8BWb2DNAE/KO7P1L4BmZ2PXA9wBFHHDGcsYuMuHjMmDO5gjkF9+t+39nzgeD+4alEjK5MD/vaumhs66axrYuGli4aWrvY09rJ3pYuLjxmBmcvnMrLDa280tDGdx9/mfefs4AVa3fy+qOmsqelkw07W9jX3s0/XLKYE+ZM5ruPv8zS+XVkepw3nzibH6/cyr3Pvco3rj2d8mSc7kwPDa1d7GzqYHtjO7uaO2npTNPaGdR+djR10NjaxaY9LTS0dOHAP195HJke54XXmtjV3Mnulk5eeK2Zva2ddGcG/sO0PBljSmXQT1NVlqClM83UqhRHTq3knKOmMbWqjKqyBNXlCarKElSVJ7RYYwmUsgbxduBSd/9AuP1e4Ex3vyHcjgG/B65z9y1m9iC9NYgyoMrd95rZacD/AMe5e1N/76cahMjYMVBfh7vT0pmmqSNNe1cGM9jT3EljJOk1tgbP97V10dSRpjwZZ39bFxt2tdDWTw0plYhRmYpTmYxTnopTkQwfqfyf5ck4lZGy6Hb0vMpwX0WkzAkSOQTL1AM0d6TZ2dTBuh3NnHZkHa/ua2fz3lYaW7s4+6hp7GzqYGdTB3tbujh9wRS27Glld3MnFx47g9f2tfO7F3cxvbosF09Zsvf9ypOxvLjKI/vKkrF+m/iGYrRqENuBeZHtuWFZVjVwPPBg+AFnAsvN7Ep3Xwl0Arj7U2b2ErAYUAYQGQcG6gg3M6rLk7kOf4Cj6qsG9bqd6QzrdjTT3JGmuSNNS2ealo5uWjrTNHem6ejK0NaVob07Q0d38LylM83u5k7auzO0R/YdqBZTKBk3ujNOdXkCA9q6MgM2+QXW557FDHoe6N3zHw9szJWbGZkDvlZfZsGEztOOrOM7f3XmgU8YolImiCeBRWa2gCAxXAO8O7vT3fcD07LbBTWIeqDB3TNmthBYBGwqYawiMg6UJeKcOHfysLxWd6YnSBaRpFJ0O0w0+9q6ScSMfe1d9DhUlyeoKU+SjBvTqsqYW1fJuh1NTK0q48S5tZgZq7fvZ1ZtOTXlSaZVl7HqlX3MrC2jMpVg9fb9tHdncndIjMbT0d2T9/658nSG9q6eXHlHmPRm1pZmIEDJEoS7p83sBuB+gmGuy9x9jZndCqx09+UDnP5G4FYz6wZ6gA+6e0OpYhWRw08yHiMZjw3rLPgzFuQvwVLYn/SGRbm/iZldsK8U8RwqTZQTETmMDdQHoW5/EREpSglCRESKUoIQEZGilCBERKQoJQgRESlKCUJERIpSghARkaImzDwIM9sNvHwILzEN2DNM4QwnxTU0imtoxmpcMHZjm2hxHenu9cV2TJgEcajMbGV/k0VGk+IaGsU1NGM1Lhi7sR1OcamJSUREilKCEBGRopQget052gH0Q3ENjeIamrEaF4zd2A6buNQHISIiRakGISIiRSlBiIhIUYd9gjCzS81snZltNLMbRzmWLWb2vJmtMrOVYdkUM1thZhvCn3UjFMsyM9tlZqsjZUVjscBXw2v4nJmdOsJx3WJm28PrtsrMLo/suymMa52ZXVLCuOaZ2QNmttbM1pjZ34flo3rNBohrVK+ZmZWb2RNm9mwY1z+H5QvM7E/h+//QzFJheVm4vTHcP3+E47rLzDZHrtfJYfmI/dsP3y9uZs+Y2S/C7dJeL3c/bB8Ed7p7CVgIpIBngWNHMZ4twLSCss8BN4bPbwQ+O0KxvBE4FVh9oFiAy4FfAQacBfxphOO6heB2tYXHHhv+TsuABeHvOl6iuGYBp4bPqwluRnzsaF+zAeIa1WsWfu6q8HkS+FN4HX4EXBOW3wH8bfj8fwN3hM+vAX5YouvVX1x3AW8vcvyI/dsP3++jwPeBX4TbJb1eh3sN4gxgo7tvcvcu4G7gqlGOqdBVwLfC598Crh6JN3X3h4HC27z2F8tVwLc98Dgw2cxmjWBc/bkKuNvdO919M7CR4Hdeirhec/enw+fNwAvAHEb5mg0QV39G5JqFn7sl3EyGDwcuAO4JywuvV/Y63gO8ycxsBOPqz4j92zezucAVwNfDbaPE1+twTxBzgK2R7W0M/J+n1Bz4jZk9ZWbXh2Uz3P218PkOYMbohDZgLGPhOt4QVvGXRZrhRiWusDp/CsFfn2PmmhXEBaN8zcLmklXALmAFQW1ln7uni7x3Lq5w/35g6kjE5e7Z6/Xp8Hp9yczKCuMqEvNw+zLwcaAn3J5Kia/X4Z4gxpo3uPupwGXA35nZG6M7PagvjolxyWMpFuBrwFHAycBrwBdHKxAzqwJ+Avwfd2+K7hvNa1YkrlG/Zu6ecfeTgbkEtZQlIx1DMYVxmdnxwE0E8Z0OTAH+70jGZGZvBna5+1Mj+b6He4LYDsyLbM8Ny0aFu28Pf+4Cfkbwn2Zntsoa/tw1WvENEMuoXkd33xn+p+4B/pveJpERjcvMkgRfwt9z95+GxaN+zYrFNVauWRjLPuAB4GyCJppEkffOxRXurwX2jlBcl4ZNde7uncA3GfnrdQ5wpZltIWgKvwD4CiW+Xod7gngSWBSOBEgRdOYsH41AzGySmVVnnwMXA6vDeK4ND7sW+PloxBfqL5blwPvCER1nAfsjzSolV9Dm++cE1y0b1zXhiI4FwCLgiRLFYMA3gBfc/d8iu0b1mvUX12hfMzOrN7PJ4fMK4CKC/pEHgLeHhxVer+x1fDvw+7BGNhJxvRhJ8kbQzh+9XiX/Pbr7Te4+193nE3xP/d7d/5JSX6/h7GEfjw+CUQjrCdo/PzGKcSwkGD3yLLAmGwtBu+HvgA3Ab4EpIxTPDwiaHroJ2jb/qr9YCEZw3B5ew+eBpSMc13fC930u/I8xK3L8J8K41gGXlTCuNxA0Hz0HrAofl4/2NRsgrlG9ZsCJwDPh+68GPhn5f/AEQef4j4GysLw83N4Y7l84wnH9Prxeq4Hv0jvSacT+7UdiPI/eUUwlvV5aakNERIo63JuYRESkH0oQIiJSlBKEiIgUpQQhIiJFKUGIiEhRShAyoszsD+HP+Wb27mF+7f9X7L1KxcyuNrNPlui1Ww581EG97nnZlUAP4TW2mNm0AfbfbWaLDuU9ZGxQgpAR5e6vD5/OB4aUICIzRvuTlyAi71UqHwf+81BfZBCfq+SGOYavEVwbGeeUIGRERf4yvg04N1xb/yPhAmmfN7MnwwXR/iY8/jwze8TMlgNrw7L/CRc0XJNd1NDMbgMqwtf7XvS9wlmunzez1Rbcb+MvIq/9oJndY2Yvmtn3sitemtltFtxD4Tkz+0KRz7EY6HT3PeH2XWZ2h5mtNLP14do52YXfBvW5irzHpy24L8HjZjYj8j5vjxzTEnm9/j7LpWHZ08BbI+feYmbfMbPHgO+Es4h/Esb6pJmdEx431cx+E17vrxNMDsvO/v9lGOPq7HUFHgEuHAuJTw5RqWf96aFH9AG0hD/PI5wNGm5fD/xj+LwMWElwP4LzgFZgQeTY7GzkCoKZrVOjr13kvd5GsFponGA11VcI7pNwHsEql3MJ/lj6I8HM46kEs4izE0knF/kc7we+GNm+C/h1+DqLCGZ5lw/lcxW8vgNvCZ9/LvIadxG5L0HB9Sz2WcoJVvVcRPDF/iN6Z+HeAjwFVITb3ydYMBLgCILlOQC+Su+M4ivC2KaF1/W/I7HURp6vAE4b7X9vehzaQzUIGSsuJljTZhXBctRTCb7UAJ7w4N4EWR82s2eBxwkWJDtQe/cbgB94sDjdTuAhglU5s6+9zYNF61YRNH3tBzqAb5jZW4G2Iq85C9hdUPYjd+9x9w3AJoLVP4fyuaK6gGxfwVNhXAdS7LMsATa7+wYPvrm/W3DOcndvD59fCPxHGOtyoMaCVWDfmD3P3X8JNIbHPw9cZGafNbNz3X1/5HV3AbMHEbOMYaoCylhhwIfc/f68QrPzCP7Sjm5fCJzt7m1m9iDBX8kHqzPyPAMk3D1tZmcAbyJY6OwGgtUzo9oJVsiMKly3xhnk5yqiO/xCz8UVPk8TNg2bWYzgToj9fpYBXj8rGkMMOMvdOwpiLXqiu6+34BablwP/Yma/c/dbw93lBNdIxjHVIGS0NBPcAjPrfuBvLViaGjNbbMGqtoVqgcYwOSwhuM1jVnf2/AKPAH8R9gfUE/xF3O8KpeFfzbXufh/wEeCkIoe9ABxdUPYOM4uZ2VEEi6itG8LnGqwtwGnh8ysJ7ng2kBeB+WFMAO8a4NjfAB/Kblh432XgYcIBBWZ2GZC9r/ZsoM3dvwt8nuBWsFmL6V3xVMYp1SBktDwHZMKmorsI1rafDzwddq7upvjtVX8NfNDMXiD4An48su9O4Dkze9qDpZCzfkZwr4FnCf6q/7i77wgTTDHVwM/NrJygBvDRIsc8DHzRzCzyl/4rBImnBvigu3eEnbqD+VyD9d9hbM8SXIuBaiGEMVwP/NLM2giSZXU/h38YuN3MniP4bngY+CDwz8APzGwN8IfwcwKcAHzezHoIVtf9W4CwQ73d3Xcc/MeUsUCruYocJDP7CnCvu//WzO4i6Py95wCnTXhm9hGgyd2/MdqxyKFRE5PIwftXoHK0gxiD9gHfGu0g5NCpBiEiIkWpBiEiIkUpQYiISFFKECIiUpQShIiIFKUEISIiRf1/9AP2LPlmcyoAAAAASUVORK5CYII=\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_dimensions = [8, 3, 5, 1]\n",
    "initialise_parameters(layer_dimensions)\n",
    "parameters = L_layer_model(X, Y.transpose(),layer_dimensions, learning_rate=0.06, num_iterations = 40000, print_cost=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.36363636363636\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.,\n        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n        1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n        1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n        1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n        0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n        1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0.,\n        0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1.,\n        1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n        1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0.,\n        0., 0.]])"
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv(file_path_test)\n",
    "X_embarked = df_test.Embarked.map(lambda df: replace_embarked(df))\n",
    "X_gender = df_test.Sex.map(lambda df: replace_gender(df))\n",
    "X_name = df_test.Name.map(lambda df: replace_name(df))\n",
    "X_class = df_test.Pclass\n",
    "X_sib = df_test.SibSp\n",
    "X_parch = df_test.Parch\n",
    "X_fare = df_test.Fare\n",
    "X_age = df_test.Age\n",
    "\n",
    "X = pd.concat([X_gender, X_class, X_name, X_embarked, X_sib, X_fare, X_parch, X_age], axis=1)\n",
    "X = np.array(X).transpose()\n",
    "#print(X.shape)\n",
    "Y = df_gender.Survived\n",
    "Y = np.array(Y)\n",
    "# print(Y.shape)\n",
    "predict(X, Y, parameters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}